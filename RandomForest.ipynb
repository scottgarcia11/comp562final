{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forest\n",
    "#### Classification of Leaves\n",
    "##### Travis Barnett\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      margin1   margin2   margin3   margin4   margin5   margin6   margin7  \\\n",
      "604  0.003906  0.007812  0.046875  0.029297  0.039062  0.039062  0.027344   \n",
      "37   0.048828  0.060547  0.003906  0.000000  0.000000  0.142580  0.039062   \n",
      "652  0.033203  0.101560  0.011719  0.003906  0.001953  0.087891  0.027344   \n",
      "886  0.031250  0.023438  0.037109  0.023438  0.009766  0.027344  0.058594   \n",
      "483  0.027344  0.041016  0.048828  0.017578  0.005859  0.041016  0.074219   \n",
      "158  0.013672  0.015625  0.017578  0.009766  0.011719  0.013672  0.035156   \n",
      "154  0.000000  0.000000  0.000000  0.021484  0.035156  0.000000  0.007812   \n",
      "40   0.011719  0.023438  0.091797  0.023438  0.013672  0.027344  0.003906   \n",
      "310  0.025391  0.064453  0.046875  0.005859  0.000000  0.058594  0.041016   \n",
      "546  0.000000  0.000000  0.019531  0.015625  0.013672  0.001953  0.013672   \n",
      "337  0.050781  0.042969  0.011719  0.035156  0.001953  0.048828  0.017578   \n",
      "523  0.007812  0.005859  0.011719  0.003906  0.013672  0.013672  0.041016   \n",
      "440  0.000000  0.000000  0.021484  0.013672  0.011719  0.007812  0.007812   \n",
      "596  0.005859  0.009766  0.019531  0.005859  0.007812  0.001953  0.025391   \n",
      "686  0.013672  0.023438  0.019531  0.011719  0.003906  0.015625  0.023438   \n",
      "18   0.001953  0.000000  0.015625  0.031250  0.011719  0.000000  0.021484   \n",
      "805  0.033203  0.156250  0.029297  0.001953  0.001953  0.164060  0.000000   \n",
      "520  0.001953  0.000000  0.037109  0.048828  0.007812  0.000000  0.001953   \n",
      "413  0.037109  0.115230  0.000000  0.003906  0.000000  0.080078  0.001953   \n",
      "982  0.017578  0.074219  0.015625  0.013672  0.000000  0.074219  0.005859   \n",
      "230  0.013672  0.017578  0.113280  0.005859  0.007812  0.015625  0.021484   \n",
      "62   0.078125  0.066406  0.019531  0.007812  0.000000  0.130860  0.021484   \n",
      "79   0.000000  0.000000  0.000000  0.005859  0.029297  0.000000  0.001953   \n",
      "672  0.005859  0.033203  0.103520  0.015625  0.001953  0.023438  0.019531   \n",
      "193  0.001953  0.001953  0.017578  0.029297  0.003906  0.000000  0.000000   \n",
      "181  0.003906  0.003906  0.021484  0.027344  0.015625  0.000000  0.021484   \n",
      "311  0.000000  0.000000  0.068359  0.064453  0.015625  0.000000  0.000000   \n",
      "247  0.003906  0.003906  0.027344  0.003906  0.013672  0.019531  0.003906   \n",
      "200  0.000000  0.000000  0.011719  0.009766  0.062500  0.000000  0.023438   \n",
      "608  0.023438  0.103520  0.041016  0.005859  0.000000  0.173830  0.000000   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "72   0.056641  0.056641  0.013672  0.021484  0.000000  0.117190  0.017578   \n",
      "845  0.027344  0.009766  0.035156  0.011719  0.003906  0.001953  0.072266   \n",
      "537  0.001953  0.013672  0.041016  0.029297  0.007812  0.011719  0.017578   \n",
      "677  0.009766  0.000000  0.056641  0.001953  0.003906  0.019531  0.044922   \n",
      "849  0.015625  0.003906  0.070312  0.005859  0.007812  0.029297  0.027344   \n",
      "973  0.011719  0.005859  0.001953  0.003906  0.011719  0.011719  0.031250   \n",
      "174  0.052734  0.101560  0.023438  0.007812  0.000000  0.082031  0.003906   \n",
      "87   0.007812  0.007812  0.027344  0.001953  0.027344  0.003906  0.009766   \n",
      "551  0.035156  0.060547  0.046875  0.003906  0.000000  0.107420  0.019531   \n",
      "486  0.033203  0.033203  0.023438  0.027344  0.001953  0.039062  0.017578   \n",
      "705  0.060547  0.125000  0.007812  0.001953  0.000000  0.156250  0.001953   \n",
      "314  0.003906  0.007812  0.031250  0.025391  0.013672  0.001953  0.029297   \n",
      "396  0.000000  0.000000  0.015625  0.003906  0.041016  0.000000  0.011719   \n",
      "600  0.001953  0.000000  0.003906  0.007812  0.017578  0.000000  0.025391   \n",
      "472  0.021484  0.017578  0.050781  0.007812  0.011719  0.021484  0.025391   \n",
      "70   0.037109  0.076172  0.044922  0.001953  0.000000  0.126950  0.000000   \n",
      "599  0.013672  0.035156  0.044922  0.017578  0.031250  0.062500  0.054688   \n",
      "804  0.050781  0.037109  0.029297  0.005859  0.000000  0.082031  0.044922   \n",
      "754  0.009766  0.000000  0.017578  0.017578  0.037109  0.000000  0.009766   \n",
      "277  0.035156  0.099609  0.039062  0.000000  0.001953  0.146480  0.017578   \n",
      "723  0.001953  0.000000  0.017578  0.027344  0.031250  0.001953  0.007812   \n",
      "9    0.000000  0.000000  0.009766  0.037109  0.072266  0.000000  0.000000   \n",
      "359  0.000000  0.003906  0.050781  0.052734  0.005859  0.005859  0.001953   \n",
      "707  0.005859  0.007812  0.025391  0.029297  0.001953  0.007812  0.007812   \n",
      "763  0.056641  0.029297  0.009766  0.011719  0.001953  0.033203  0.056641   \n",
      "835  0.000000  0.000000  0.111330  0.060547  0.005859  0.000000  0.003906   \n",
      "192  0.007812  0.019531  0.083984  0.015625  0.001953  0.017578  0.000000   \n",
      "629  0.048828  0.105470  0.015625  0.007812  0.001953  0.105470  0.009766   \n",
      "559  0.000000  0.000000  0.009766  0.011719  0.064453  0.000000  0.003906   \n",
      "684  0.000000  0.000000  0.083984  0.111330  0.013672  0.000000  0.000000   \n",
      "\n",
      "      margin8   margin9  margin10    ...      texture55  texture56  texture57  \\\n",
      "604  0.000000  0.003906  0.023438    ...       0.000000   0.000000   0.004883   \n",
      "37   0.000000  0.007812  0.046875    ...       0.011719   0.002930   0.004883   \n",
      "652  0.000000  0.009766  0.017578    ...       0.189450   0.000000   0.034180   \n",
      "886  0.000000  0.005859  0.056641    ...       0.000000   0.000000   0.000000   \n",
      "483  0.000000  0.007812  0.056641    ...       0.003906   0.000000   0.041016   \n",
      "158  0.003906  0.000000  0.021484    ...       0.107420   0.000000   0.010742   \n",
      "154  0.000000  0.003906  0.001953    ...       0.149410   0.000000   0.002930   \n",
      "40   0.000000  0.007812  0.015625    ...       0.074219   0.000000   0.041016   \n",
      "310  0.000000  0.005859  0.025391    ...       0.000000   0.000000   0.000000   \n",
      "546  0.000000  0.011719  0.007812    ...       0.000000   0.000000   0.000977   \n",
      "337  0.000000  0.005859  0.031250    ...       0.000000   0.000000   0.000000   \n",
      "523  0.000000  0.000000  0.013672    ...       0.027344   0.000000   0.041016   \n",
      "440  0.000000  0.005859  0.005859    ...       0.017578   0.000000   0.025391   \n",
      "596  0.000000  0.003906  0.015625    ...       0.024414   0.000000   0.041016   \n",
      "686  0.000000  0.003906  0.013672    ...       0.034180   0.000000   0.049805   \n",
      "18   0.001953  0.005859  0.013672    ...       0.000000   0.000000   0.001953   \n",
      "805  0.000000  0.009766  0.019531    ...       0.000000   0.004883   0.000000   \n",
      "520  0.000000  0.009766  0.005859    ...       0.005859   0.008789   0.016602   \n",
      "413  0.005859  0.000000  0.013672    ...       0.000000   0.000000   0.001953   \n",
      "982  0.000000  0.007812  0.001953    ...       0.025391   0.000000   0.003906   \n",
      "230  0.005859  0.001953  0.013672    ...       0.018555   0.000000   0.011719   \n",
      "62   0.000000  0.000000  0.046875    ...       0.282230   0.000000   0.008789   \n",
      "79   0.000000  0.000000  0.003906    ...       0.000000   0.006836   0.000000   \n",
      "672  0.000000  0.001953  0.015625    ...       0.000000   0.000000   0.000000   \n",
      "193  0.007812  0.000000  0.005859    ...       0.045898   0.000000   0.008789   \n",
      "181  0.000000  0.003906  0.015625    ...       0.000000   0.000000   0.001953   \n",
      "311  0.000000  0.005859  0.000000    ...       0.015625   0.007812   0.020508   \n",
      "247  0.005859  0.007812  0.013672    ...       0.000000   0.000977   0.000000   \n",
      "200  0.005859  0.007812  0.005859    ...       0.034180   0.001953   0.003906   \n",
      "608  0.000000  0.003906  0.007812    ...       0.312500   0.000000   0.000977   \n",
      "..        ...       ...       ...    ...            ...        ...        ...   \n",
      "72   0.000000  0.003906  0.031250    ...       0.067383   0.000000   0.002930   \n",
      "845  0.003906  0.001953  0.042969    ...       0.000000   0.000000   0.015625   \n",
      "537  0.001953  0.000000  0.017578    ...       0.000977   0.000000   0.007812   \n",
      "677  0.005859  0.000000  0.027344    ...       0.088867   0.000000   0.006836   \n",
      "849  0.000000  0.000000  0.015625    ...       0.094727   0.000000   0.044922   \n",
      "973  0.000000  0.000000  0.021484    ...       0.066406   0.000000   0.096680   \n",
      "174  0.000000  0.005859  0.050781    ...       0.045898   0.002930   0.000000   \n",
      "87   0.000000  0.013672  0.011719    ...       0.014648   0.000000   0.050781   \n",
      "551  0.003906  0.003906  0.019531    ...       0.008789   0.000000   0.002930   \n",
      "486  0.000000  0.001953  0.048828    ...       0.000000   0.000000   0.000000   \n",
      "705  0.000000  0.005859  0.013672    ...       0.101560   0.000000   0.027344   \n",
      "314  0.003906  0.000000  0.009766    ...       0.000000   0.000000   0.000000   \n",
      "396  0.000000  0.017578  0.000000    ...       0.141600   0.016602   0.006836   \n",
      "600  0.000000  0.000000  0.009766    ...       0.000000   0.001953   0.011719   \n",
      "472  0.000000  0.000000  0.023438    ...       0.000000   0.000000   0.000000   \n",
      "70   0.000000  0.005859  0.021484    ...       0.045898   0.000977   0.037109   \n",
      "599  0.000000  0.003906  0.027344    ...       0.000000   0.000000   0.006836   \n",
      "804  0.000000  0.001953  0.039062    ...       0.056641   0.000000   0.078125   \n",
      "754  0.000000  0.005859  0.009766    ...       0.000000   0.000000   0.000000   \n",
      "277  0.000000  0.011719  0.025391    ...       0.000000   0.000000   0.000000   \n",
      "723  0.000000  0.009766  0.000000    ...       0.000000   0.000000   0.000000   \n",
      "9    0.000000  0.007812  0.001953    ...       0.000000   0.000000   0.000000   \n",
      "359  0.000000  0.044922  0.001953    ...       0.098633   0.041992   0.000977   \n",
      "707  0.000000  0.009766  0.023438    ...       0.000000   0.000000   0.003906   \n",
      "763  0.000000  0.007812  0.048828    ...       0.000000   0.000000   0.000000   \n",
      "835  0.000000  0.005859  0.000000    ...       0.052734   0.024414   0.023438   \n",
      "192  0.000000  0.050781  0.000000    ...       0.099609   0.042969   0.002930   \n",
      "629  0.000000  0.009766  0.017578    ...       0.041016   0.000000   0.051758   \n",
      "559  0.000000  0.001953  0.003906    ...       0.000000   0.000977   0.000000   \n",
      "684  0.000000  0.005859  0.000000    ...       0.008789   0.000000   0.000977   \n",
      "\n",
      "     texture58  texture59  texture60  texture61  texture62  texture63  \\\n",
      "604   0.000977   0.015625   0.000000   0.000000   0.000000   0.000000   \n",
      "37    0.000000   0.005859   0.000000   0.000000   0.038086   0.000000   \n",
      "652   0.000000   0.018555   0.000000   0.000000   0.000000   0.000000   \n",
      "886   0.136720   0.005859   0.017578   0.000000   0.126950   0.018555   \n",
      "483   0.003906   0.014648   0.000000   0.000000   0.014648   0.043945   \n",
      "158   0.000000   0.008789   0.000000   0.000000   0.000000   0.027344   \n",
      "154   0.002930   0.001953   0.000000   0.000000   0.001953   0.000000   \n",
      "40    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "310   0.011719   0.039062   0.000000   0.000000   0.000977   0.000000   \n",
      "546   0.004883   0.002930   0.005859   0.000000   0.014648   0.000000   \n",
      "337   0.135740   0.002930   0.073242   0.000000   0.137700   0.019531   \n",
      "523   0.002930   0.024414   0.000000   0.000000   0.000000   0.000000   \n",
      "440   0.000000   0.023438   0.000000   0.000000   0.007812   0.027344   \n",
      "596   0.000000   0.006836   0.000000   0.000000   0.000000   0.015625   \n",
      "686   0.000977   0.017578   0.000000   0.000000   0.000000   0.003906   \n",
      "18    0.005859   0.049805   0.000000   0.000000   0.006836   0.000000   \n",
      "805   0.035156   0.000000   0.441410   0.000000   0.017578   0.000000   \n",
      "520   0.002930   0.014648   0.000000   0.002930   0.012695   0.004883   \n",
      "413   0.000000   0.039062   0.000000   0.000000   0.000000   0.049805   \n",
      "982   0.000000   0.009766   0.000000   0.012695   0.000000   0.009766   \n",
      "230   0.000000   0.012695   0.000000   0.000000   0.000000   0.003906   \n",
      "62    0.000977   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "79    0.050781   0.031250   0.015625   0.000000   0.074219   0.003906   \n",
      "672   0.011719   0.023438   0.000000   0.000000   0.000977   0.000000   \n",
      "193   0.000000   0.006836   0.000000   0.000000   0.000000   0.004883   \n",
      "181   0.002930   0.013672   0.000000   0.000000   0.001953   0.008789   \n",
      "311   0.002930   0.021484   0.000000   0.008789   0.002930   0.000977   \n",
      "247   0.013672   0.000000   0.487300   0.000000   0.090820   0.010742   \n",
      "200   0.004883   0.016602   0.000000   0.000000   0.049805   0.000000   \n",
      "608   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "..         ...        ...        ...        ...        ...        ...   \n",
      "72    0.000000   0.002930   0.000000   0.023438   0.000000   0.009766   \n",
      "845   0.000000   0.006836   0.000000   0.000000   0.000000   0.000000   \n",
      "537   0.003906   0.011719   0.000000   0.000000   0.005859   0.047852   \n",
      "677   0.000000   0.008789   0.000000   0.000000   0.000000   0.008789   \n",
      "849   0.016602   0.036133   0.000000   0.000000   0.003906   0.000000   \n",
      "973   0.000000   0.006836   0.000000   0.000000   0.000000   0.030273   \n",
      "174   0.005859   0.005859   0.000000   0.037109   0.033203   0.002930   \n",
      "87    0.000000   0.020508   0.000000   0.000000   0.000000   0.000000   \n",
      "551   0.000977   0.007812   0.000000   0.000000   0.001953   0.006836   \n",
      "486   0.115230   0.005859   0.139650   0.000000   0.118160   0.000000   \n",
      "705   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "314   0.002930   0.029297   0.000000   0.000000   0.001953   0.028320   \n",
      "396   0.002930   0.020508   0.000000   0.000000   0.022461   0.002930   \n",
      "600   0.006836   0.022461   0.000000   0.000000   0.025391   0.007812   \n",
      "472   0.100590   0.056641   0.000000   0.000000   0.059570   0.000000   \n",
      "70    0.005859   0.018555   0.000000   0.000977   0.004883   0.003906   \n",
      "599   0.000000   0.056641   0.000000   0.004883   0.000000   0.020508   \n",
      "804   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "754   0.000977   0.000977   0.005859   0.000000   0.009766   0.000000   \n",
      "277   0.011719   0.000000   0.298830   0.000000   0.017578   0.000000   \n",
      "723   0.000000   0.000000   0.026367   0.000000   0.028320   0.000000   \n",
      "9     0.070312   0.013672   0.192380   0.000000   0.074219   0.000000   \n",
      "359   0.001953   0.028320   0.000000   0.038086   0.004883   0.000977   \n",
      "707   0.000977   0.015625   0.000000   0.000000   0.000000   0.017578   \n",
      "763   0.123050   0.000977   0.019531   0.000000   0.173830   0.017578   \n",
      "835   0.000977   0.025391   0.000000   0.017578   0.000977   0.000977   \n",
      "192   0.002930   0.021484   0.000000   0.000000   0.027344   0.000000   \n",
      "629   0.005859   0.007812   0.000000   0.000000   0.015625   0.000000   \n",
      "559   0.045898   0.047852   0.019531   0.000000   0.059570   0.001953   \n",
      "684   0.000977   0.027344   0.000000   0.000000   0.010742   0.017578   \n",
      "\n",
      "     texture64  \n",
      "604   0.006836  \n",
      "37    0.000000  \n",
      "652   0.003906  \n",
      "886   0.000000  \n",
      "483   0.043945  \n",
      "158   0.001953  \n",
      "154   0.011719  \n",
      "40    0.015625  \n",
      "310   0.000977  \n",
      "546   0.000000  \n",
      "337   0.000000  \n",
      "523   0.013672  \n",
      "440   0.041016  \n",
      "596   0.013672  \n",
      "686   0.023438  \n",
      "18    0.001953  \n",
      "805   0.000000  \n",
      "520   0.000000  \n",
      "413   0.004883  \n",
      "982   0.000000  \n",
      "230   0.034180  \n",
      "62    0.016602  \n",
      "79    0.000977  \n",
      "672   0.000000  \n",
      "193   0.010742  \n",
      "181   0.040039  \n",
      "311   0.011719  \n",
      "247   0.000000  \n",
      "200   0.001953  \n",
      "608   0.009766  \n",
      "..         ...  \n",
      "72    0.000977  \n",
      "845   0.023438  \n",
      "537   0.106450  \n",
      "677   0.017578  \n",
      "849   0.040039  \n",
      "973   0.011719  \n",
      "174   0.029297  \n",
      "87    0.005859  \n",
      "551   0.036133  \n",
      "486   0.000000  \n",
      "705   0.003906  \n",
      "314   0.000977  \n",
      "396   0.007812  \n",
      "600   0.048828  \n",
      "472   0.000000  \n",
      "70    0.038086  \n",
      "599   0.001953  \n",
      "804   0.001953  \n",
      "754   0.000000  \n",
      "277   0.000000  \n",
      "723   0.000000  \n",
      "9     0.000000  \n",
      "359   0.008789  \n",
      "707   0.011719  \n",
      "763   0.000000  \n",
      "835   0.023438  \n",
      "192   0.008789  \n",
      "629   0.062500  \n",
      "559   0.006836  \n",
      "684   0.027344  \n",
      "\n",
      "[792 rows x 192 columns]\n",
      "[15 39 20 96 38 26 54 28 17 79 13 68 71 23 76 14 47 34 23 88 94 26 30  7\n",
      " 16 35 92 27 86 10  3 28 28 82 43  2 54 93 26 90 18 67 70 78 81  8 19 63\n",
      " 84 38  7 14 42 60 55 57 92 28 77 74 42 70 66 71 88 40 78 73 57 33 11 58\n",
      " 13 58 40  5 98 10 32 98 60  1 34 63  1 10 60 90 12 50 53 61 67 20 59 49\n",
      " 36 19 76 68 67 80 24 49 31 76 10 46  5 17  0 90 59 29 37 61  8 22 58 26\n",
      " 85 72 37 13 32 61 11  8 31 22  6 52 98 32  7 12 24  2 20 30 93 27 68  8\n",
      " 89 82 36 80  1 40 11 93 74 56 27  3 54 21 57 54  5 52 38 37 31 88 88 41\n",
      " 64 77 70  5 20 94 83 82 31 81 59 83  5 74 74 62 89 68 54  0 87 96 85 93\n",
      " 11 50 63 71 21 17]\n"
     ]
    }
   ],
   "source": [
    "#extract data and create train/test subsets\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "X = train.iloc[:,2:]\n",
    "species = train.iloc[:,1]\n",
    "y = pd.factorize(species)[0]\n",
    "\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.7min\n",
      "/Users/Travis/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 18.0min finished\n",
      "/Users/Travis/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 2000, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 90, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "#search for best parameters using random search\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter=100, cv=3, verbose=2, random_state=0, n_jobs=-1)\n",
    "rf_random.fit(xTrain, yTrain)\n",
    "\n",
    "print (rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   37.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed: 11.3min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 16.5min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed: 18.4min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 20.4min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed: 24.4min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed: 26.6min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 28.9min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed: 31.4min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed: 33.9min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 36.9min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed: 39.9min\n",
      "[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed: 42.8min\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed: 45.0min finished\n",
      "/Users/Travis/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=80, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=2500, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.9494949494949495\n"
     ]
    }
   ],
   "source": [
    "#Conduct grid search with cross validation based on results from random search\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [1000, 1500, 2000, 2500],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'max_depth': [80,90,100],\n",
    "    'bootstrap': [True],\n",
    "    'min_samples_split': [2,3,5],\n",
    "    'min_samples_leaf': [1,2]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,cv=3,n_jobs=-1, verbose=10)\n",
    "grid_search.fit(xTrain,yTrain)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "print(best_grid)\n",
    "best_grid.fit(xTrain,yTrain)\n",
    "pred = best_grid.predict(xTest)\n",
    "grid_accuracy = accuracy_score(pred,yTest)\n",
    "print('Accuracy = ', grid_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:   12.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Test accuracy 0.9444444444444444 Train Accuracy:  1.0\n",
      "2: Test accuracy 0.9494949494949495 Train Accuracy:  1.0\n",
      "3: Test accuracy 0.9545454545454546 Train Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "#Explore three possible hyperparameters found using grid search\n",
    "\n",
    "best_param = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=80, max_features='sqrt', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=3,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=1200, n_jobs=None,\n",
    "            oob_score=False, random_state=0, verbose=1,\n",
    "            warm_start=False)\n",
    "\n",
    "best_param1 = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=80, max_features='sqrt', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=2, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=1500, n_jobs=None,\n",
    "            oob_score=False, random_state=0, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "best_param2=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=80, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
    "            oob_score=False, random_state=0, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "best_param.fit(xTrain,yTrain)\n",
    "pred = best_param.predict(xTest)\n",
    "train_accuracy = accuracy_score(yTrain, best_param.predict(xTrain))\n",
    "test_accuracy = accuracy_score(pred, yTest)\n",
    "\n",
    "best_param1.fit(xTrain,yTrain)\n",
    "pred1 = best_param1.predict(xTest)\n",
    "train_accuracy1 = accuracy_score(yTrain, best_param1.predict(xTrain))\n",
    "test_accuracy1 = accuracy_score(pred1, yTest)\n",
    "\n",
    "best_param2.fit(xTrain,yTrain)\n",
    "pred2 = best_param2.predict(xTest)\n",
    "train_accuracy2 = accuracy_score(yTrain, best_param2.predict(xTrain))\n",
    "test_accuracy2 = accuracy_score(pred2, yTest)\n",
    "\n",
    "print(\"1: Test accuracy\", test_accuracy, \"Train Accuracy: \",train_accuracy)\n",
    "print(\"2: Test accuracy\", test_accuracy1, \"Train Accuracy: \",train_accuracy1)\n",
    "print(\"3: Test accuracy\", test_accuracy2, \"Train Accuracy: \",train_accuracy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default train accuracy 1.0\n",
      "Default test accuracy  0.8383838383838383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Travis/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Explore accuracy using default RF hyperparameters\n",
    "\n",
    "default_param = RandomForestClassifier()\n",
    "default_param.fit(xTrain,yTrain)\n",
    "def_pred = default_param.predict(xTest)\n",
    "def_accuracy = accuracy_score(def_pred,yTest)\n",
    "print('Default train accuracy', accuracy_score(yTrain, default_param.predict(xTrain)))\n",
    "print('Default test accuracy ', def_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9545454545454546\n",
      "Feature ranking:\n",
      "1. feature 47 (0.009877)\n",
      "2. feature 136 (0.008910)\n",
      "3. feature 2 (0.008909)\n",
      "4. feature 54 (0.008200)\n",
      "5. feature 5 (0.008032)\n",
      "6. feature 112 (0.007722)\n",
      "7. feature 79 (0.007679)\n",
      "8. feature 186 (0.007601)\n",
      "9. feature 80 (0.007560)\n",
      "10. feature 12 (0.007524)\n",
      "11. feature 140 (0.007492)\n",
      "12. feature 42 (0.007481)\n",
      "13. feature 58 (0.007400)\n",
      "14. feature 128 (0.007382)\n",
      "15. feature 176 (0.007342)\n",
      "16. feature 113 (0.007336)\n",
      "17. feature 40 (0.007324)\n",
      "18. feature 1 (0.007319)\n",
      "19. feature 147 (0.007199)\n",
      "20. feature 133 (0.007136)\n",
      "21. feature 49 (0.007010)\n",
      "22. feature 141 (0.006991)\n",
      "23. feature 129 (0.006963)\n",
      "24. feature 81 (0.006898)\n",
      "25. feature 111 (0.006874)\n",
      "26. feature 78 (0.006822)\n",
      "27. feature 157 (0.006795)\n",
      "28. feature 3 (0.006747)\n",
      "29. feature 110 (0.006742)\n",
      "30. feature 158 (0.006655)\n",
      "31. feature 64 (0.006637)\n",
      "32. feature 0 (0.006632)\n",
      "33. feature 17 (0.006567)\n",
      "34. feature 38 (0.006500)\n",
      "35. feature 169 (0.006473)\n",
      "36. feature 82 (0.006403)\n",
      "37. feature 135 (0.006310)\n",
      "38. feature 161 (0.006219)\n",
      "39. feature 53 (0.006202)\n",
      "40. feature 65 (0.006142)\n",
      "41. feature 29 (0.006124)\n",
      "42. feature 127 (0.006110)\n",
      "43. feature 20 (0.006079)\n",
      "44. feature 45 (0.006071)\n",
      "45. feature 44 (0.006064)\n",
      "46. feature 126 (0.006044)\n",
      "47. feature 177 (0.006025)\n",
      "48. feature 191 (0.006022)\n",
      "49. feature 165 (0.006019)\n",
      "50. feature 46 (0.005989)\n",
      "51. feature 156 (0.005962)\n",
      "52. feature 97 (0.005924)\n",
      "53. feature 109 (0.005904)\n",
      "54. feature 114 (0.005886)\n",
      "55. feature 95 (0.005864)\n",
      "56. feature 21 (0.005845)\n",
      "57. feature 37 (0.005838)\n",
      "58. feature 68 (0.005800)\n",
      "59. feature 160 (0.005791)\n",
      "60. feature 125 (0.005760)\n",
      "61. feature 77 (0.005742)\n",
      "62. feature 31 (0.005724)\n",
      "63. feature 83 (0.005721)\n",
      "64. feature 66 (0.005700)\n",
      "65. feature 30 (0.005687)\n",
      "66. feature 96 (0.005685)\n",
      "67. feature 172 (0.005682)\n",
      "68. feature 181 (0.005660)\n",
      "69. feature 155 (0.005656)\n",
      "70. feature 74 (0.005636)\n",
      "71. feature 61 (0.005621)\n",
      "72. feature 189 (0.005606)\n",
      "73. feature 98 (0.005534)\n",
      "74. feature 120 (0.005524)\n",
      "75. feature 99 (0.005522)\n",
      "76. feature 122 (0.005520)\n",
      "77. feature 150 (0.005514)\n",
      "78. feature 175 (0.005511)\n",
      "79. feature 130 (0.005501)\n",
      "80. feature 185 (0.005455)\n",
      "81. feature 119 (0.005453)\n",
      "82. feature 121 (0.005450)\n",
      "83. feature 71 (0.005408)\n",
      "84. feature 124 (0.005318)\n",
      "85. feature 94 (0.005265)\n",
      "86. feature 123 (0.005240)\n",
      "87. feature 93 (0.005215)\n",
      "88. feature 4 (0.005213)\n",
      "89. feature 14 (0.005208)\n",
      "90. feature 69 (0.005193)\n",
      "91. feature 48 (0.005177)\n",
      "92. feature 182 (0.005170)\n",
      "93. feature 73 (0.005138)\n",
      "94. feature 131 (0.005123)\n",
      "95. feature 13 (0.005114)\n",
      "96. feature 67 (0.005106)\n",
      "97. feature 92 (0.005105)\n",
      "98. feature 43 (0.004981)\n",
      "99. feature 23 (0.004928)\n",
      "100. feature 9 (0.004916)\n",
      "101. feature 27 (0.004908)\n",
      "102. feature 162 (0.004893)\n",
      "103. feature 75 (0.004880)\n",
      "104. feature 84 (0.004878)\n",
      "105. feature 72 (0.004869)\n",
      "106. feature 91 (0.004864)\n",
      "107. feature 153 (0.004855)\n",
      "108. feature 8 (0.004854)\n",
      "109. feature 170 (0.004837)\n",
      "110. feature 118 (0.004826)\n",
      "111. feature 134 (0.004826)\n",
      "112. feature 88 (0.004799)\n",
      "113. feature 105 (0.004783)\n",
      "114. feature 39 (0.004778)\n",
      "115. feature 171 (0.004756)\n",
      "116. feature 146 (0.004746)\n",
      "117. feature 139 (0.004745)\n",
      "118. feature 24 (0.004742)\n",
      "119. feature 57 (0.004739)\n",
      "120. feature 188 (0.004736)\n",
      "121. feature 76 (0.004706)\n",
      "122. feature 144 (0.004704)\n",
      "123. feature 138 (0.004700)\n",
      "124. feature 106 (0.004683)\n",
      "125. feature 28 (0.004642)\n",
      "126. feature 6 (0.004634)\n",
      "127. feature 178 (0.004613)\n",
      "128. feature 148 (0.004610)\n",
      "129. feature 167 (0.004607)\n",
      "130. feature 100 (0.004577)\n",
      "131. feature 116 (0.004539)\n",
      "132. feature 190 (0.004532)\n",
      "133. feature 117 (0.004530)\n",
      "134. feature 90 (0.004527)\n",
      "135. feature 70 (0.004520)\n",
      "136. feature 19 (0.004502)\n",
      "137. feature 26 (0.004491)\n",
      "138. feature 18 (0.004484)\n",
      "139. feature 174 (0.004469)\n",
      "140. feature 62 (0.004461)\n",
      "141. feature 10 (0.004457)\n",
      "142. feature 115 (0.004452)\n",
      "143. feature 85 (0.004427)\n",
      "144. feature 36 (0.004420)\n",
      "145. feature 164 (0.004420)\n",
      "146. feature 89 (0.004407)\n",
      "147. feature 132 (0.004394)\n",
      "148. feature 154 (0.004356)\n",
      "149. feature 41 (0.004328)\n",
      "150. feature 173 (0.004305)\n",
      "151. feature 101 (0.004295)\n",
      "152. feature 32 (0.004292)\n",
      "153. feature 55 (0.004247)\n",
      "154. feature 50 (0.004209)\n",
      "155. feature 87 (0.004184)\n",
      "156. feature 137 (0.004172)\n",
      "157. feature 145 (0.004149)\n",
      "158. feature 187 (0.004148)\n",
      "159. feature 86 (0.004129)\n",
      "160. feature 34 (0.004097)\n",
      "161. feature 159 (0.004084)\n",
      "162. feature 184 (0.004083)\n",
      "163. feature 52 (0.004073)\n",
      "164. feature 108 (0.004069)\n",
      "165. feature 11 (0.004031)\n",
      "166. feature 107 (0.004026)\n",
      "167. feature 163 (0.003998)\n",
      "168. feature 143 (0.003962)\n",
      "169. feature 149 (0.003898)\n",
      "170. feature 104 (0.003831)\n",
      "171. feature 152 (0.003821)\n",
      "172. feature 63 (0.003784)\n",
      "173. feature 183 (0.003760)\n",
      "174. feature 102 (0.003757)\n",
      "175. feature 142 (0.003736)\n",
      "176. feature 179 (0.003723)\n",
      "177. feature 35 (0.003612)\n",
      "178. feature 103 (0.003576)\n",
      "179. feature 151 (0.003539)\n",
      "180. feature 166 (0.003346)\n",
      "181. feature 180 (0.003328)\n",
      "182. feature 56 (0.003314)\n",
      "183. feature 25 (0.003298)\n",
      "184. feature 16 (0.003184)\n",
      "185. feature 168 (0.003103)\n",
      "186. feature 59 (0.002948)\n",
      "187. feature 22 (0.002665)\n",
      "188. feature 51 (0.002170)\n",
      "189. feature 60 (0.001030)\n",
      "190. feature 7 (0.000684)\n",
      "191. feature 33 (0.000601)\n",
      "192. feature 15 (0.000181)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Explore Feature Importance for a particular model\n",
    "\n",
    "\n",
    "best_param3=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=80, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "best_param3.fit(xTrain,yTrain)\n",
    "pred3 = best_param3.predict(xTest)\n",
    "accuracy3 = accuracy_score(pred2, yTest)\n",
    "print(accuracy3)\n",
    "\n",
    "importances = best_param3.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in best_param3.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
